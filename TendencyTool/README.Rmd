---
title: "README"
author: "Mathilde Vimont"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

# Introduction

Cette suite d’outils R doit permettre d’analyser les données d’abondance d’espèces dans le but d’extraire une tendance d’évolution dans le temps. Le calcul de cette tendance passe par :

- la réalisation d’un modèle linéaire généralisé mixte, avec la variable temps/année. Le coefficient associé à 

Vous trouverez dans cette suite d’outil : 

- un ensemble de fonctions permettant la réalisation de ces types de modèle, la mise en forme des résultats et l’extraction des tendances, ainsi que la visualisation des variations d’abondance dans le temps ;

- un script `blabla.R` qui permet d’analyser en routine un ensemble d’espèces, et d’exporter proprement l’ensemble des résultats.

La suite de ce README a pour objectif de documenter les différentes fonctions mises en place dans ce package, et de donner des exemples d’utilisation de ces fonctions.

# Installer le package

```{r library, echo=TRUE}

```

# Données d'abondance

## Chargement des données

```{r data, echo=TRUE}
library(TendencyTools)
data(TendencyTools)
```

Les données sont donc maintenant disponibles dans R, sous le nom de : `data`.

## Format du jeu de données

```{r columns, echo=TRUE}
head(data)
```

Ce jeu de données contient de nombreuses informations relatives aux observations et conditions d'observation des espèces. Parmi celles nécessaires au bon déroulé de l'analyse :

- le champ `species` est obligatoire, et doit contenir le nom/l'identifiant de l'espèce observée ;

- le champ `year` est obligatoire, et doit contenir l'année d'observation de l'espèce ;

- le champ `transect` (ou `point`) est facultatif, et constitue l'échelle la plus fine d'observation. Elle est accompagnée d'un champ `site` obligatoire, qui est associé à un ou plusieurs `transect` (ou `point`) ;

-  le champ `ID` est obligatoire, et correspond à un identifiant unique **date + site (+ point/transect)** ;
 
- le champ `count` est obligatoire, et correspond à l'information d'abondance.

## Gestion des absences

Les jeux de données d'abondance peuvent parfois être transmis sans données d'absence, ou bien à cause du volume qu'elles représenteraient, ou bien parce que le protocole ne prévoit pas l'enregistrement des absences. La fonction `fillAbsence` a été imaginée pour reconstruire ces absences, lorsque les informations de passage à un site à une date (`ID`) sont disponibles :

```{r fillAbsence, include = FALSE, echo=TRUE}
dataAll <- fillAbsence(data = data, method = "once")

```

```{r nrow, echo=TRUE}
# Nombre de données sans absence
nrow(data)

# Nombre de données avec absence
nrow(dataAll)
```

# Régression & tendance générale

Pour l'exemple, on s'intéresse particulièrement à l'espèce **Lysandra coridon**.

```{r nrow, echo=TRUE}
dataSp <- dataAll[dataAll$species == "Lysandra coridon",]

# Nombre de données pour l'espèce
nrow(dataSp)
```

## Choisir la variable d'intérêt et les variables explicatives

Une des premières étapes est de choisir :

- quelle variable représente la variable d'intérêt `interestVar` ? Ici il s'agit de l'abondance stockée dans le champ *count* ;


```{r intVar, include = T}
summary(dataSp$count)

```

- quelle(s) variable(s) doivent être considérées comme des effets fixes `fixedEffects` ? Ici, nous nous intéressons particulièrement à l'effet de l'année (champ *year*) mais nous souhaitons aussi contrôler pour l'effet de la localisation au travers des champs *longitude* et *latitude* ;

```{r fixEff, include = T}
# Year
summary(dataSp$year)

# Longitude
summary(dataSp$longitude)

# Latitude
summary(dataSp$latitude)

```

- quelle(s) variable(s) doivent être considérées comme des effets aléatoires `randomEffects` ? Ici, le *site* peut avoir une influence forte sur l'abondance perçue. Il s'agit d'une variable catégorielle à beaucoup de niveaux. La considérer comme un effet aléatoire nous permet d'une part de garder de la puissance dans le calcul de tendances, et d'autre part de généraliser les résultats à l'ensemble des sites de France.

```{r randEff, echo=TRUE}
table(dataSp$site)[1:10]

```

La fonction `writeFormula` permet d'écrire la formule de régression adaptée au choix des variables précédent, ce qui donne en l'occurrence : 

```{r formula, echo=TRUE}
writeFormula(interestVar=  "count",
             fixedEffects = c("year", "longitude", "latitude"), 
             randomEffects = "site")
```

## Vérifier que les données contiennent toutes les variables

Une fois que les variables à inclure dans la régression sont définies, une petite étape de contrôle consiste à vérifier que toutes ces variables sont bien contenues dans le jeu de données étudié. Cette vérification peut être faite à partir de la fonction `checkData` :

```{r chechData, echo=TRUE}
checkData(dataSp, type = "transect", 
          interestVar=  "count",
          fixedEffects = c("year", "longitude", "latitude"), 
          randomEffects = "site")
```

## Choisir la distribution 

Une des étapes importantes de la régression est le choix de la **distribution des résidus**. Un régression linéaire classique repose sur l'hypothèse selon laquelle les résidus de la régression suivent une loi normale, ce qui est plutôt bien adapté à une variable à expliquer continue. 

Les données d'abondance peuvent être particulières, par exemple données de *présence/absence* (0/1) ou encore données de *comptage*, comme c'est le cas dans notre jeu de données, ce qui pose la question de la pertinence de la distribution normale. Les régressions linéaires généralisées permettent un ensemble d'autres distributions, dont certaines ont été implémentées dans cet outil : 

- **distribution binomiale**, particulièrement adaptée aux données de présence/absence

- **distribution de Poisson**, particulièrement adaptée aux données de comptage, mais qui impose néanmoins une contrainte très forte d'égalité entre moyenne et variance, ce qui peut entraîner des problèmes de sur-dispersion (cf. partie XXXX pour les implications) ;

- **distribution négative binomiale**, adaptée aux comptages et qui repose sur un paramètre de dispersion supplémentaire permettant de surmonter certains problèmes rencontrés avec la distribution de Poisson.

Pour commencer, nous choisirons ici comme `distribution`, celle de poisson.

## Faire la régression

Une fois que l'on a formaté les données, et que l'on a déterminé la distribution et les variables, la fonction `makeGLM` permet de lancer la régression linéaire (généralisée (mixte)). Cette fonction repose sur le package `glmmTMB`, qui a plusieurs avantages notamment :

- l'implémentation de nombreuses distributions (dont négative binomiale) ;

- des temps de calcul plutôt faible.

```{r glm, echo=TRUE}
mod <- makeGLM(data = dataSp, 
               interestVar = "count", 
               fixedEffects = c("year","longitude","latitude"),
               randomEffects = "site", 
               distribution = "poisson")  
```

La fonction doit retourner une liste de 3 éléments :

- `mod$value`, donne les résultats du (G)LM(M) si aucune erreur n'a été rencontrée pendant le processus d'estimation des paramètres ;

```{r value, echo=TRUE}
mod$value
```

- `mod$warnings`, liste l'ensemble des alertes rencontrées pendant le processus d'estimation des paramètres. Si une des erreurs concerne un **problème de convergence**, la fiabilité du modèle est à remettre en question ... ;

```{r warn, echo=TRUE}
mod$warnings
```

- `mod$error`, liste l'ensemble des erreurs rencontrées pendant le processus d'estimation des paramètres.

```{r err, echo=TRUE}
mod$error
```

*NB : le processus d'estimation des paramètres peut être perturbé par la présence de paramètres numériques avec des plages de valeurs importantes et variables d'un paramètre à l'autre (ex : température entre 0 et 40°C vs. année entre 2000 et 2020). Pour cela, une astuce simple peut être de centrer/réduire les variables numériques. Cela est possible dans la fonction `makeGLM` au travers du paramètre `scaling = TRUE`.*

```{r scaledglm, echo=TRUE}
modScaled <- makeGLM(data = dataSp, 
                     interestVar = "count", 
                     fixedEffects = c("year","longitude","latitude"),
                     randomEffects = "site", 
                     distribution = "poisson",
                     scaling = TRUE)
```

## Formatter les résultats

Une fois la régression réalisée, nous souhaitons extraire les résultats (estimations, erreurs, p-value, ...) dans un format plus lisible pour l'utilisateur. La fonction `summaryOutput` doit permettre ce reformatage et contient un certain nombre de paramètres qui permettent d'affiner plus ou moins la transformation :

- le paramètre `rescale` permet de dé-centrer/réduire les coefficients et erreurs standards. Ce paramètre n'a de sens que si les variables explicatives numériques ont été centrées-réduites dans le modèle ;

- le paramètre `transform` permet de transformer les coefficients lorsque la fonction de lien est différente de l'identité (i.e, quand il s'agit d'un log ou logit). C'est le cas pour toutes les distributions autres que la gaussienne.


Les estimations brutes sont regroupées dans le tableau suivant. 
```{r estim1, echo=TRUE}
summaryOutput(model = modScaled$value, 
              distribution = "poisson",
              transform = FALSE, 
              rescale = FALSE)
```

Les estimations dé-centrées/réduites sont regroupées dans le tableau suivant, à interpréter comme suit : quand la variable year augmente de 1, l'abondance augmente de log() : 
```{r estim2, echo=TRUE}
summaryOutput(model = modScaled$value, 
              distribution = "poisson",
              transform = FALSE, 
              rescale = TRUE)
```

Les estimations dé-centrées/réduites et retransformées sont regroupées dans le tableau suivant. 
```{r estim3, echo=TRUE}
summaryOutput(model = modScaled$value, 
              distribution = "poisson",
              transform = TRUE, 
              rescale = TRUE)
```

## Extraire la tendance

Nous cherchons à savoir plus précisément comment a évolué l'abondance de l'espèce entre la première et la dernière année d'observation. La fonction `analyseCoef` permet de calculer deux indicateurs intéressants : la tendance `trend` et le pourcentage de variation `perc`, dont l'interprétation est explicitée par la suite.

```{r coeff, echo=TRUE}
varYear <- max(dataSp$year) - min(dataSp$year) + 1

coefficients <- analyseCoef(model = modScaled$value,
                            distribution = "poisson",
                            effectVar = "year",
                            varRange = varYear)

```

Entre la première et la dernière année, l'abondance a été multipliée par :

```{r trend, echo=TRUE}
coefficients$trend
```

Entre la première et la dernière année, l'abondance a évolué de :

```{r perc, echo=TRUE}
paste(coefficients$perc, "%")
```

# Régression & tendance annuelle

## Régression 

Dans la partie précédente, nous avons calculé la tendance globale d'évolution de l'abondance. Nous cherchons maintenant à étudier les variations d'abondance qui peuvent exister d'une année sur l'autre. Pour cela, nous pouvons réaliser un (G)LM(M) où la variable **year** est considérée comme une **variable catégorielle**. Cela est rendu possible par le paramètre `factorVariables` dans la fonction `makeGLM`. 

```{r glmCat, echo=TRUE}

modCat <- makeGLM(data = dataSp, 
                  interestVar = "count", 
                  fixedEffects = c("year", "longitude", "latitude"),
                  randomEffects = "site",
                  factorVariables = "year", 
                  distribution = "poisson", 
                  scaling = T)
```

Dans ce cadre, les coefficients associés à chaque année permettront de voir les variations d'abondance par rapport à l'année de référence qu'est la première année.

```{r summaryCat, echo=TRUE}
summaryCat <- summaryOutput(model = modCat$value, 
                            distribution = "poisson", 
                            factorVariables = "year", 
                            transform = TRUE, 
                            rescale = TRUE)

summaryCat
```

## Visualisation

Maintenant que nous avons récupéré les coefficients associés à chaque année (hormis pour l'année de référence), nous aimerions les représenter sur un graphe pour visualiser les tendances d'évolution. Pour cela, la fonction `plotGLM` permet de représenter sur le même graph : les coefficients associés à chaque année, les intervalles de confiance des estimations, ainsi que la significativité de la différence entre l'abondance moyenne à l'année X et l'abondance à l'année de référence.

```{r summaryCat, echo=TRUE}
plotGLM(summary = summaryCat, effect = "year", distribution = "poisson")

```

# Contrôle de la qualité du modèle

Un certain nombre de contrôles doivent être effectués afin de s'assurer de la bonne qualité du modèle et donc des estimations. Ils sont détaillés dans cette section.

## VIF

La multi-colinéarité est un phénomène qui traduit le fait qu'une des variables explicatives est **linéairement liée aux autres variables**. Cela rend les paramètres estimés pour ces variables instables, et peut même cacher un effet significatif de ces variables.  

La fonction `measureVIF` permet de calculer le **VIF (Variance Inflation Factor)**, qui est un indicateur souvent utilisé dans le cadre de la multi-colinéarité. 

```{r vif, echo = T}
vif <- measureVIF(model = modScaled$value)

vif
```

Il n'existe pas de consensus autour d'une valeur seuil : 2 est très conservatif, 10 très peu conservatif, 5 est une valeur intermédiaire que nous recommadons ici, mais l'utilisateur est libre de gérer la multi-colinéarité comme il le souhaite.

NB : dans certains cas, une forte colinéarité n'est pas forcément un problème, c'est le cas quand il s'agit :

- de variables contrôles dont l'effet ne nous intéresse pas, et qu'elles ne sont pas colinéaires à une variable d'intérêt, elles peuvent être laissées dans le modèle ;

- de variables catégorielles avec beaucoup de niveaux, dont un avec peu d'observations ;

- de variables "polynomiales" (ex : longitude et longitude^2).

Si par contre une variable est fortement colinéaire avec une variable explicative d'intérêt, il est **judicieux de la retirer du modèle**.

## Dispersion des résidus



## Zero-Inflation



## Auto-corrélation spatiale


# Fonction annexe : gérer les erreurs

Lors d'analyses en routine sur un grand nombre d'espèces, des erreurs peuvent se produire en cours de route et arrêter les analyses. Pour maintenir les analyses, tout en gardant une traçe des erreurs / alertes, la fonction `catchConditions` permet d'encapsuler des fonctions potentiellement instables et d'extraire les alertes ou erreurs rencontrées. C'est le cas dans `makeGLM`, dans laquelle la fonction `glmmTMB` y est encapsulée.

Construisons d'abord une fonction simple, qui peut présenter des alertes ou des warnings : 

```{r fun, echo = FALSE}
# Fonction permettant de calculer la somme de x et y
fun <- function(x, y){
  
  # Alerte si l'une des deux variables est NA
  if(is.na(x)|is.na(y)){
    warning("NAs will be produced")
  }
  else{
    # Erreur si l'une des deux variables n'est pas numérique
    if(!is.numeric(x)|!is.numeric(y)){
      stop("x and y should be numeric values")
    }
  }
  
  return(x + y)
}

```

Le premier exemple montre le résultat de l'encapsulation lorsque le déroulé de la fonction se produit sans alerte ni erreur : dans ce cas là, seul le champ `value` renvoie une valeur non `NULL`, qui correspond bien au résultat de la fonction :

```{r catchCond_OK, include = TRUE}
catchConditions(expr = fun(x = 5, y = 2))

```

Le deuxième exemple montre le résultat de l'encapsulation lorsqu'une alerte se produit pendant le déroulé de la fonction : dans ce cas là, le champ `value` renvoie une valeur qui correspond au résultat de la fonction, et le champ `warnings` renvoie le message d'alerte affiché pendant la procédure : 

```{r catchCond_W, include = TRUE}
catchConditions(expr = fun(x = 5, y = NA))

```

Le dernier exemple montre le résultat de l'encapsulation lorsqu'une erreur se produit pendant le déroulé de la fonction : dans ce cas là, seul le champ `error` est non `NULL`, et contient le message d'erreur affiché pendant la procédure. 

*NB : le résultat ne peut pas être calculé, pour autant l'erreur ne stoppe plus le processus !*

```{r catchCond_PBM, include = TRUE}
catchConditions(expr = fun(x = 5, y = "2"))

```
